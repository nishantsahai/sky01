{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "710ab1f9-d0cf-4c76-b04a-04cfac970fbc",
   "metadata": {},
   "source": [
    "# **Step 1: Using pysqliste3 instead of sqlite3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "940374c6-96bd-417a-b67a-b642ddf213b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pysqlite3\n",
    "sys.modules[\"sqlite3\"] = sys.modules.pop(\"pysqlite3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118c2e05-74b5-421e-ae04-9460555b9dfd",
   "metadata": {},
   "source": [
    "# **Step 2: Importing the Necessary Modules**\n",
    "\n",
    "a) Importing from langchain framework modules for loading pdf file, splitting the text, Chroma DB for storing text in embedding form, prompt templates,\n",
    "LLM Chain for chaining prompts and ChatOpenAI for chat conversion.  \n",
    "b) Importing OS module for reading environment variable from localhost.  \n",
    "c) Importing Openai for embeddings module.  \n",
    "d) Importing dotenv for loading environment variable.  \n",
    "e) Importing gradio for generating user friendly user interface for passing query to chatbot and for AI response.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7664ffb-b106-4e2d-b194-796de033ab8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8e7bc0-0ae1-4115-a5a5-455c69885c63",
   "metadata": {},
   "source": [
    "# **Step 3: Setting Open API Key**\n",
    "\n",
    "Loading environment variables for OpenAI API key. This key is required to authenticate the request while making a call to OpenAI for embedding model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba74191d-0398-4300-b10b-fcc83876d2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = load_dotenv(find_dotenv())  # Read local .env file\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a8e319-d860-4232-8430-5340849fd168",
   "metadata": {},
   "source": [
    "# **Step 4: Loading PDFs Using PyPDFLoader**\n",
    "\n",
    "Loading the dataset. In this case, the Nestle HR Policy document. Then splitting based on pages and printing page 0.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "246fe28d-231f-4254-9d9d-4e9e0ce388d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Policy\n",
      "MandatorySeptember   2012\n",
      "The Nestlé  \n",
      "Human Resources Policy' metadata={'source': 'the_nestle_hr_policy_pdf_2012.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "pdf_loader = PyPDFLoader(\"the_nestle_hr_policy_pdf_2012.pdf\")\n",
    "pdf_pages = pdf_loader.load_and_split()\n",
    "print(pdf_pages[0])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7366467a-09de-4537-9c97-33bfa26478da",
   "metadata": {},
   "source": [
    "# **Step 5: Spliting the Documents Using RecursiveCharacterTextSplitter**\n",
    "\n",
    "Splitting text into chucks. This helps in keeping the token within the max length of different transformer models. This also increases performance by parallel processing.  \n",
    "Here fixing the chuck size to 1024 and having little overlap.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "18e6f435-e1f3-4c35-9f4a-6d25f4392524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "#Spliting text into chunks for processing\n",
    "doc_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=64)\n",
    "split_texts = doc_splitter.split_documents(pdf_pages)\n",
    "#Printing length of the split texts\n",
    "print(len(split_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8ebb3d-d1b0-482b-9b4a-1dce7e26c63b",
   "metadata": {},
   "source": [
    "# **Step 6: Embedding the Documents Using OpenAIEmbeddings and Print the Length of the Embedding**\n",
    "\n",
    "Using OpenAI Embeddings for the text  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "34965ed8-bc38-485d-b973-26364b5e2770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536\n"
     ]
    }
   ],
   "source": [
    "# Embedding the text using OpenAI embeddings\n",
    "text = split_texts[0].page_content\n",
    "openai_embed = OpenAIEmbeddings()\n",
    "openai_embed_result = openai_embed.embed_documents([text])\n",
    "#Printing the length of the embedding after text is embedded. \n",
    "print(len(openai_embed_result[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85698f1-4913-4f7b-80ab-428f32eac7c0",
   "metadata": {},
   "source": [
    "# **Step 7: Creating a Chroma Instance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "387c9e79-79c5-4344-b949-e17db6053c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a vector store using Chroma and adding embeddings\n",
    "chroma_db = Chroma.from_documents(split_texts, openai_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cec094-ba32-4f28-8747-474ac122abef",
   "metadata": {},
   "source": [
    "# **Step 8: Defining Prompt Template**\n",
    "\n",
    "Defining a prompt template.  \n",
    "Here asking AI to respond to the query that is asked by user based on the text supplied to it. \n",
    "The text is obtained from Chroma VectorDB based on query asking search matching technique and then it is passed to AI. This text serves as the content for the AI chatbot.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ac784c8-f7b2-4017-bad6-34b5b4e1a240",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = \"\"\"\n",
    "You are a HR manager of a company. You will be provided with the text. A question will be asked. \n",
    "Your task is to answer the question based on the text provided. \n",
    "\n",
    "TEXT: {text}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "ANSWER:\n",
    "\"\"\"\n",
    "\n",
    "#Passing 'text' and 'question' as input variabled for the prompt. The text is the search content obtained from ChromaDB and question is the query asked by user.\n",
    "prompt = PromptTemplate(input_variables=[\"text\", \"question\"], template=chat_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55114782-df80-49c6-b9b1-1b41e183aa87",
   "metadata": {},
   "source": [
    "# **Step 9: Initializing the Chatbot using ChatOpenAI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "53c23607-784c-4ec1-a728-24d7e08b8492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing ChatOpenAI model, using 'GPT-3.5-Turbo\n",
    "chat_model = ChatOpenAI(model=\"gpt-3.5-turbo\", openai_api_key=openai.api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da61875-0be9-44a0-bafd-c9d23826815c",
   "metadata": {},
   "source": [
    "# **Step 10: Defining the chain with the prompt and the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9b08e542-9d3a-404c-be8a-ee23ebbb0406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the chain with the prompt and model\n",
    "scenario_chain = LLMChain(llm=chat_model, prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c6de42-800c-4a77-9745-0085d5e30616",
   "metadata": {},
   "source": [
    "# **Step 11: Creating OpenAI Chatbot response**\n",
    "\n",
    "A function to perform a similarity search on Chrome DB to obtain the query response. The top 2 results based on probability score are picked here and returned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "19d5e2b3-b8a0-41fd-b1f5-a64595b09408",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot_response(query):\n",
    "    # Performing a similarity search in Chroma DB\n",
    "    search_results = chroma_db.similarity_search(query, k=2)\n",
    "    if search_results:\n",
    "        relevant_text = \" \".join([result.page_content for result in search_results])\n",
    "    else:\n",
    "        relevant_text = \"No relevant information found.\"\n",
    "    \n",
    "    # Using the chain to generate an answer based on the retrieved text\n",
    "    response = scenario_chain.run({\"text\": relevant_text, \"question\": query})\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cb6169-7bf5-4ffa-bf67-c3a3e912f4ac",
   "metadata": {},
   "source": [
    "# **Step 12: Integrating with Gradio interface**\n",
    "\n",
    "Integrating with Gardio. This creates a user friendly interface, where user can type their question in input field and AI chatbot share the response in output field.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a6de15-a5f3-4739-b509-b1d950d3f725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=chatbot_response, \n",
    "    inputs=\"text\", \n",
    "    outputs=\"text\", \n",
    "    title=\"Nestle HR Policy Chatbot\",\n",
    "    description=\"Ask questions about Nestle's HR policies and get answers based on the document!\"\n",
    ")\n",
    "\n",
    "# Launching the Gradio app. The 'share' parameter is used to generate public sharable URL (active for 72 hours) since this project is done in online lab. \n",
    "iface.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ec6d4c-6e49-4ef3-9768-5e698a7f7591",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 [3.10]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
